{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time \n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    'lpep_pickup_datetime',\n",
    "    'lpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'tip_amount'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully sent all data to Kafka in 83 seconds.\n"
     ]
    }
   ],
   "source": [
    "topic_name = 'green-trips'\n",
    "csv_file = 'data/green_tripdata_2019-10.csv'  # change to your CSV file path if needed\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(csv_file, 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        # Each row will be a dictionary keyed by the CSV headers\n",
    "        # Send data to Kafka topic \"green-data\"\n",
    "\n",
    "        # Handle missing or empty passenger_count\n",
    "        passenger_count = row['passenger_count'].strip() if row['passenger_count'] else None\n",
    "        passenger_count = int(passenger_count) if passenger_count is not None else 0\n",
    "\n",
    "        filtered_row = {\n",
    "                'lpep_pickup_datetime': row['lpep_pickup_datetime'],\n",
    "                'lpep_dropoff_datetime': row['lpep_dropoff_datetime'],\n",
    "                'PULocationID': int(row['PULocationID']),\n",
    "                'DOLocationID': int(row['DOLocationID']),\n",
    "                'passenger_count': passenger_count,\n",
    "                'trip_distance': float(row['trip_distance']),\n",
    "                'tip_amount': float(row['tip_amount']),\n",
    "            }\n",
    "        #filtered_row = {key: row[key] for key in selected_columns}\n",
    "\n",
    "        producer.send(topic_name, value=filtered_row)\n",
    "\n",
    "# Make sure any remaining messages are delivered\n",
    "producer.flush()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "producer.close()\n",
    "\n",
    "# Calculate the time taken in seconds (rounded to a whole number)\n",
    "time_taken_seconds = round(end_time - start_time)\n",
    "\n",
    "print(f\"✅ Successfully sent all data to Kafka in {time_taken_seconds} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lpep_pickup_datetime': '2019-10-01 00:26:02', 'lpep_dropoff_datetime': '2019-10-01 00:39:58', 'PULocationID': '112', 'DOLocationID': '196', 'passenger_count': '1', 'trip_distance': '5.88', 'tip_amount': '0'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(csv_file, 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        # Each row will be a dictionary keyed by the CSV headers\n",
    "        # Send data to Kafka topic \"green-data\"\n",
    "        filtered_row = {key: row[key] for key in selected_columns}\n",
    "        print(filtered_row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'test_data': 10, 'event_timestamp': 1741754906112.9846}\n",
      "Sent: {'test_data': 11, 'event_timestamp': 1741754906177.9329}\n",
      "Sent: {'test_data': 12, 'event_timestamp': 1741754906235.293}\n",
      "Sent: {'test_data': 13, 'event_timestamp': 1741754906293.6414}\n",
      "Sent: {'test_data': 14, 'event_timestamp': 1741754906344.523}\n",
      "Sent: {'test_data': 15, 'event_timestamp': 1741754906401.7617}\n",
      "Sent: {'test_data': 16, 'event_timestamp': 1741754906459.3242}\n",
      "Sent: {'test_data': 17, 'event_timestamp': 1741754906522.2498}\n",
      "Sent: {'test_data': 18, 'event_timestamp': 1741754906575.216}\n",
      "Sent: {'test_data': 19, 'event_timestamp': 1741754906638.1377}\n",
      "Sent: {'test_data': 20, 'event_timestamp': 1741754906690.4878}\n",
      "Sent: {'test_data': 21, 'event_timestamp': 1741754906747.06}\n",
      "Sent: {'test_data': 22, 'event_timestamp': 1741754906808.5146}\n",
      "Sent: {'test_data': 23, 'event_timestamp': 1741754906864.9182}\n",
      "Sent: {'test_data': 24, 'event_timestamp': 1741754906927.5684}\n",
      "Sent: {'test_data': 25, 'event_timestamp': 1741754906993.3826}\n",
      "Sent: {'test_data': 26, 'event_timestamp': 1741754907058.9402}\n",
      "Sent: {'test_data': 27, 'event_timestamp': 1741754907125.2183}\n",
      "Sent: {'test_data': 28, 'event_timestamp': 1741754907177.0115}\n",
      "Sent: {'test_data': 29, 'event_timestamp': 1741754907236.7566}\n",
      "Sent: {'test_data': 30, 'event_timestamp': 1741754907286.894}\n",
      "Sent: {'test_data': 31, 'event_timestamp': 1741754907344.886}\n",
      "Sent: {'test_data': 32, 'event_timestamp': 1741754907402.9946}\n",
      "Sent: {'test_data': 33, 'event_timestamp': 1741754907461.2905}\n",
      "Sent: {'test_data': 34, 'event_timestamp': 1741754907520.2751}\n",
      "Sent: {'test_data': 35, 'event_timestamp': 1741754907583.801}\n",
      "Sent: {'test_data': 36, 'event_timestamp': 1741754907643.5032}\n",
      "Sent: {'test_data': 37, 'event_timestamp': 1741754907706.2197}\n",
      "Sent: {'test_data': 38, 'event_timestamp': 1741754907762.764}\n",
      "Sent: {'test_data': 39, 'event_timestamp': 1741754907820.3672}\n",
      "Sent: {'test_data': 40, 'event_timestamp': 1741754907875.3164}\n",
      "Sent: {'test_data': 41, 'event_timestamp': 1741754907927.554}\n",
      "Sent: {'test_data': 42, 'event_timestamp': 1741754907991.1067}\n",
      "Sent: {'test_data': 43, 'event_timestamp': 1741754908046.6858}\n",
      "Sent: {'test_data': 44, 'event_timestamp': 1741754908109.1748}\n",
      "Sent: {'test_data': 45, 'event_timestamp': 1741754908160.6436}\n",
      "Sent: {'test_data': 46, 'event_timestamp': 1741754908216.694}\n",
      "Sent: {'test_data': 47, 'event_timestamp': 1741754908273.247}\n",
      "Sent: {'test_data': 48, 'event_timestamp': 1741754908327.8657}\n",
      "Sent: {'test_data': 49, 'event_timestamp': 1741754908386.7998}\n",
      "Sent: {'test_data': 50, 'event_timestamp': 1741754908441.897}\n",
      "Sent: {'test_data': 51, 'event_timestamp': 1741754908494.1406}\n",
      "Sent: {'test_data': 52, 'event_timestamp': 1741754908550.1812}\n",
      "Sent: {'test_data': 53, 'event_timestamp': 1741754908601.585}\n",
      "Sent: {'test_data': 54, 'event_timestamp': 1741754908662.733}\n",
      "Sent: {'test_data': 55, 'event_timestamp': 1741754908719.6392}\n",
      "Sent: {'test_data': 56, 'event_timestamp': 1741754908783.9263}\n",
      "Sent: {'test_data': 57, 'event_timestamp': 1741754908847.6396}\n",
      "Sent: {'test_data': 58, 'event_timestamp': 1741754908900.811}\n",
      "Sent: {'test_data': 59, 'event_timestamp': 1741754908959.7166}\n",
      "Sent: {'test_data': 60, 'event_timestamp': 1741754909013.9548}\n",
      "Sent: {'test_data': 61, 'event_timestamp': 1741754909076.2646}\n",
      "Sent: {'test_data': 62, 'event_timestamp': 1741754909136.061}\n",
      "Sent: {'test_data': 63, 'event_timestamp': 1741754909196.736}\n",
      "Sent: {'test_data': 64, 'event_timestamp': 1741754909251.4023}\n",
      "Sent: {'test_data': 65, 'event_timestamp': 1741754909310.0881}\n",
      "Sent: {'test_data': 66, 'event_timestamp': 1741754909373.44}\n",
      "Sent: {'test_data': 67, 'event_timestamp': 1741754909431.5776}\n",
      "Sent: {'test_data': 68, 'event_timestamp': 1741754909485.7932}\n",
      "Sent: {'test_data': 69, 'event_timestamp': 1741754909542.9976}\n",
      "Sent: {'test_data': 70, 'event_timestamp': 1741754909600.354}\n",
      "Sent: {'test_data': 71, 'event_timestamp': 1741754909653.4758}\n",
      "Sent: {'test_data': 72, 'event_timestamp': 1741754909719.371}\n",
      "Sent: {'test_data': 73, 'event_timestamp': 1741754909772.4392}\n",
      "Sent: {'test_data': 74, 'event_timestamp': 1741754909829.2878}\n",
      "Sent: {'test_data': 75, 'event_timestamp': 1741754909892.5146}\n",
      "Sent: {'test_data': 76, 'event_timestamp': 1741754909949.7856}\n",
      "Sent: {'test_data': 77, 'event_timestamp': 1741754910012.794}\n",
      "Sent: {'test_data': 78, 'event_timestamp': 1741754910076.4375}\n",
      "Sent: {'test_data': 79, 'event_timestamp': 1741754910139.231}\n",
      "Sent: {'test_data': 80, 'event_timestamp': 1741754910194.1765}\n",
      "Sent: {'test_data': 81, 'event_timestamp': 1741754910245.6138}\n",
      "Sent: {'test_data': 82, 'event_timestamp': 1741754910299.0652}\n",
      "Sent: {'test_data': 83, 'event_timestamp': 1741754910353.7532}\n",
      "Sent: {'test_data': 84, 'event_timestamp': 1741754910408.2922}\n",
      "Sent: {'test_data': 85, 'event_timestamp': 1741754910463.8198}\n",
      "Sent: {'test_data': 86, 'event_timestamp': 1741754910526.485}\n",
      "Sent: {'test_data': 87, 'event_timestamp': 1741754910587.285}\n",
      "Sent: {'test_data': 88, 'event_timestamp': 1741754910647.5078}\n",
      "Sent: {'test_data': 89, 'event_timestamp': 1741754910703.7424}\n",
      "Sent: {'test_data': 90, 'event_timestamp': 1741754910760.372}\n",
      "Sent: {'test_data': 91, 'event_timestamp': 1741754910824.3257}\n",
      "Sent: {'test_data': 92, 'event_timestamp': 1741754910880.027}\n",
      "Sent: {'test_data': 93, 'event_timestamp': 1741754910934.2039}\n",
      "Sent: {'test_data': 94, 'event_timestamp': 1741754910990.4902}\n",
      "Sent: {'test_data': 95, 'event_timestamp': 1741754911055.2454}\n",
      "Sent: {'test_data': 96, 'event_timestamp': 1741754911113.1873}\n",
      "Sent: {'test_data': 97, 'event_timestamp': 1741754911174.9673}\n",
      "Sent: {'test_data': 98, 'event_timestamp': 1741754911239.3323}\n",
      "Sent: {'test_data': 99, 'event_timestamp': 1741754911294.482}\n",
      "took 5.25 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10, 100):\n",
    "    message = {'test_data': i, 'event_timestamp': time.time() * 1000}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
